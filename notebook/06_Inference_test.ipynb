{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57213baa-7f97-45bd-be31-efc434716323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 06: Inference Testing\n",
    "\n",
    "**Purpose:** This notebook tests the MLflow model we logged in the previous script. We will test inference in two ways:\n",
    "\n",
    "1.  **Batch Inference (Spark UDF):** Simulates a large-scale batch scoring job on a Spark DataFrame.\n",
    "2.  **Real-time Inference (Pandas):** Simulates a single request, such as one coming from a REST API, using a Pandas DataFrame.\n",
    "\n",
    "This script confirms that the logged artifact (model + image processor) is a complete, deployable pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d94bed50-51e6-4aba-8ea7-047a0c3174f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "**Reason:** We must ensure the inference environment is identical to the training environment to prevent errors.\n",
    "\n",
    "* `libaio-dev`: A system-level library required by `transformers`/`accelerate` on this cluster's OS (Ubuntu Noble). Without it, PyTorch fails to initialize.\n",
    "* `transformers==4.49.0`: We pin this exact version to match what the model was trained with and what `mlflow==2.21.3` is compatible with.\n",
    "* `uv`: This Python installer is now required by `mlflow` to manage virtual environments for UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e048e7-b2c0-457a-a1ef-8b226be875fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Update package lists and install the required AIO library\n",
    "sudo apt-get update && sudo apt-get install -y libaio-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46991f3f-4aa7-419f-8437-66181274adbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pin libraries to the versions used in training and logging\n",
    "%pip install transformers==4.49.0\n",
    "%pip install uv\n",
    "\n",
    "# Restart the Python kernel for the new libraries to take effect\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ff6d01-f1c0-438d-8174-cc245b5eff93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import base64\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset, ClassLabel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the MLflow Run ID from notebook 05 (Manual Logging)\n",
    "RUN_ID = \"6ffbb8dafe5e4395b1c93d2547ad160a\" \n",
    "MODEL_URI = f\"runs:/{RUN_ID}/model\"\n",
    "\n",
    "# This is the Delta table we created in notebook 04\n",
    "DATASET_NAME = \"train_dataset\" \n",
    "\n",
    "# add the metadata to enable the image preview\n",
    "img_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\":\"image/jpeg\"}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874c8d60-425a-4499-b3fe-a79db9ed28c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Define Preprocessing Functions\n",
    "\n",
    "The logged MLflow model is a *pipeline* that includes the `image_processor`. This pipeline expects the raw image, not the `pixel_values` tensor.\n",
    "\n",
    "We will define two functions to convert our raw image `bytes` into a `base64 string`, which the pipeline accepts.\n",
    "1.  `bytes_to_base64_udf`: A Spark UDF for batch inference.\n",
    "2.  `bytes_to_base64_pd`: A regular Python function for Pandas inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7e8d3-91bf-402c-a95b-f13cc5f0ef39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def bytes_to_base64_udf(image_bytes):\n",
    "    \"\"\"\n",
    "    Spark UDF: Converts raw image bytes into a Base64-encoded string.\n",
    "    \"\"\"\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "def bytes_to_base64_pd(image_bytes):\n",
    "    \"\"\"\n",
    "    Python function for Pandas: Converts raw image bytes into a Base64-encoded string.\n",
    "    \"\"\"\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c6e887-a1e2-4ce1-99cf-415896a5bbc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Test 1: Batch Inference with Spark UDF\n",
    "\n",
    "Here, we test scoring on a sample of the Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd971f0-f501-4284-aedb-427ddd036a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Spark DataFrame...\")\n",
    "df = spark.read.table(DATASET_NAME)\n",
    "\n",
    "# Get 5 'noise' samples for testing\n",
    "sample_df = df.filter(col('label') == 'noise').limit(5)\n",
    "\n",
    "# 1. Preprocess the data: Convert 'image' bytes to 'image_base64' string\n",
    "inference_df = sample_df.withColumn(\"image_base64\", bytes_to_base64_udf(col(\"image\")))\n",
    "\n",
    "# 2. Load the model as a Spark UDF\n",
    "# We must specify result_type=StringType() because the model's 'task' \n",
    "# (image-classification) makes it output a JSON-like string, not a float.\n",
    "print(\"Loading model as Spark UDF...\")\n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=MODEL_URI, \n",
    "    result_type=StringType()\n",
    ")\n",
    "\n",
    "# 3. Run prediction\n",
    "# We call the UDF *only* on the 'image_base64' column.\n",
    "print(\"Running batch prediction on 5 samples...\")\n",
    "predictions_df = inference_df.withColumn(\n",
    "    'predictions', \n",
    "    loaded_model_udf(col(\"image_base64\"))\n",
    ")\n",
    "\n",
    "predictions_df.withColumn('image', col('image').alias('image', metadata = img_meta))\n",
    "display(predictions_df.select(\"name\", \"image\", \"label\", \"predictions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f409c053-346a-4172-8901-5963c6be318a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Test 2: Real-time Inference with Pandas\n",
    "\n",
    "Here, we test scoring using `pyfunc.load_model` and a Pandas DataFrame. This simulates how a REST API (like Databricks Model Serving) would use the model for a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a26330d-0192-41b6-8b8c-dcc131fb8d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the model as a standard PyFunc object\n",
    "print(\"Loading PyFunc model...\")\n",
    "pyfunc_model = mlflow.pyfunc.load_model(MODEL_URI)\n",
    "\n",
    "# 2. Get 5 'surfing' (normal) samples and 5 'noise' (anomaly) samples\n",
    "print(\"Preparing Pandas DataFrames...\")\n",
    "normal_pd_df = df.filter(col('label') == 'surfing').limit(5).toPandas()\n",
    "anomaly_pd_df = df.filter(col('label') == 'noise').limit(5).toPandas()\n",
    "\n",
    "# 3. Prepare the input data\n",
    "# The pipeline expects a DataFrame where each row is one base64 string.\n",
    "normal_input_data = pd.DataFrame(normal_pd_df['image'].apply(bytes_to_base64_pd))\n",
    "anomaly_input_data = pd.DataFrame(anomaly_pd_df['image'].apply(bytes_to_base64_pd))\n",
    "\n",
    "# 4. Run predictions\n",
    "print(\"\\n--- Predicting on NORMAL (surfing) samples ---\")\n",
    "normal_predictions = pyfunc_model.predict(normal_input_data)\n",
    "display(normal_predictions)\n",
    "\n",
    "print(\"\\n--- Predicting on ANOMALY (noise) samples ---\")\n",
    "anomaly_predictions = pyfunc_model.predict(anomaly_input_data)\n",
    "display(anomaly_predictions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7690907141378352,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Inference_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

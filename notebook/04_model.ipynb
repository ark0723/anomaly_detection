{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaec8768-5d2f-470e-a79c-09335d2be6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Training (ViT Anomaly Detection)\n",
    "\n",
    "This notebook trains a Vision Transformer (ViT) model for anomaly detection. The process involves:\n",
    "\n",
    "1.  **Environment Setup**: Installing system-level dependencies (`libaio`) and specific Python libraries (`transformers==4.49.0`) to ensure compatibility and fix bugs.\n",
    "2.  **Authentication Setup**: Configuring the Spark session with Azure Key Vault secrets to allow MLflow to access storage for data lineage.\n",
    "3.  **Data Preparation**: Loading data from a Delta table, converting it to a Hugging Face `Dataset`, and applying preprocessing.\n",
    "4.  **Model Definition**: Loading a pre-trained `google/vit-base-patch16-224` and adapting its classification head for our binary (normal/anomaly) task.\n",
    "5.  **Training Setup**: Defining `TrainingArguments` and a custom `compute_metrics` function for our imbalanced dataset (using F1-score).\n",
    "6.  **Training & Manual Logging**: Running the `Trainer` and, due to `autolog()` unreliability in this environment, manually logging the final model, processor, and data lineage to MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097dde6d-c8e0-403a-bdac-daaed435fe67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Environment Setup: System Dependencies\n",
    "\n",
    "We must first install `libaio-dev` (Linux Asynchronous I/O development library).\n",
    "\n",
    "**Reason**: The `transformers` and `accelerate` libraries, when running on certain Linux environments like this (Ubuntu Noble), have a deep dependency on this system library. Without it, the `Trainer` will fail to initialize and crash with a cryptic `ld: cannot find -laio` error. This is a system-level fix, not a Python one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b92576-8efb-44da-a077-2b2b9865251e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Update package lists and install the required AIO library\n",
    "sudo apt-get update && sudo apt-get install -y libaio-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f4ccf7a-5248-43d8-b631-9c040b2f74bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Environment Setup: Python Libraries\n",
    "\n",
    "Next, we install specific Python libraries:\n",
    "\n",
    "* `azure-identity` & `azure-keyvault-secrets`: Required to authenticate with Azure Key Vault and retrieve the secrets (e.g., Service Principal credentials) needed to access our storage.\n",
    "* `transformers==4.49.0`: **This version pin is critical.** Our `mlflow==2.21.3` version is only officially compatible with `transformers <= 4.49.0`. Using a newer version (like 4.50.x) caused `mlflow.transformers.autolog()` to **silently fail**â€”it would log parameters and metrics, but **fail to save the final model artifact.** Downgrading ensures compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d392593-3363-410b-b7c8-48dd79d3a3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-identity azure-keyvault-secrets\n",
    "\n",
    "# Pin transformers to a version compatible with our MLflow version (2.21.3)\n",
    "%pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b4b67e-8514-4c65-bd88-7195cbb203e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After installing new libraries, we must restart the Python kernel \n",
    "# for the changes to take effect in the notebook's environment.\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a48b137-fec4-4dc2-b9ea-e72fe6993755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow, transformers\n",
    "# Verify the MLflow and transformers version.\n",
    "print(mlflow.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1933bca3-b40b-422e-9d3a-c80706afe0c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Environment Setup: Spark Authentication\n",
    "\n",
    "**Reason**: This step is required for **MLflow Data Lineage** (`mlflow.log_input`). When we call `mlflow.data.from_spark(train_df)`, MLflow tries to launch a *new* Spark job to profile the data. This new job needs permission to read the original Delta table files from `abfss://` storage.\n",
    "\n",
    "The credentials used to mount `/mnt/datamount` are not automatically inherited by this new MLflow job. Therefore, we must *explicitly* set the Service Principal credentials (fetched from Key Vault) on the **global Spark session configuration**. This ensures any Spark job spawned by this session (including MLflow's) has the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c15a159-1d7d-42b9-a26c-485cd53006de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Fetch secrets from Azure Key Vault ---\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "key_vault_url = \"https://anomalykeyvault10222025.vault.azure.net/\"\n",
    "key_name_tenant_id = \"tenant-id\"\n",
    "key_name_client_id = \"client-id\"\n",
    "key_name_client_secret = \"client-secret\"\n",
    "storge_account_name = \"stlabelingdevwestus001\" # The storage account we need to access\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=key_vault_url, credential=credential)\n",
    "\n",
    "tenant_id = client.get_secret(key_name_tenant_id).value\n",
    "client_id = client.get_secret(key_name_client_id).value\n",
    "client_secret = client.get_secret(key_name_client_secret).value\n",
    "\n",
    "# --- 2. Set Service Principal credentials on the global Spark session ---\n",
    "# This allows MLflow's background jobs to authenticate to Azure Data Lake Storage (ADLS).\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storge_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storge_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storge_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storge_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storge_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "print(f\"Spark session configured for Service Principal access to: {storge_account_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcfe3e0-8877-4638-9cb6-83ce2664aac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define and verify the mount path for our data\n",
    "mount_path = \"/mnt/datamount\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a54785-3636-4819-a007-4181d98ee26d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Load the augmented image dataset from the Parquet files\n",
    "print(\"Loading original DataFrame...\")\n",
    "df = spark.read.format('parquet').load(f'{mount_path}/images_augmented')\n",
    "\n",
    "# Define the name for Delta table\n",
    "dataset_name = \"train_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a3649e-c937-47f9-9707-8caa55399eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Delta table for reliability and versioning.\n",
    "# This is the source table MLflow will track for lineage.\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d39379-c67e-4d3b-979a-f5dfcedf979a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Hugging Face datasets library\n",
    "from datasets import Dataset, ClassLabel\n",
    "\n",
    "# Load the Delta table back as a Spark DataFrame\n",
    "train_df = spark.read.table(dataset_name)\n",
    "\n",
    "# Convert the Spark DataFrame to a Hugging Face Dataset object\n",
    "# This loads the data into memory/cache for use by the Trainer.\n",
    "train_dataset = Dataset.from_spark(train_df)\n",
    "\n",
    "# Define the class labels. 'surfing' will be 0, 'noise' will be 1.\n",
    "class_label_feature = ClassLabel(names=['surfing', 'noise'])\n",
    "print(\"Casting 'label' column to ClassLabel...\")\n",
    "\n",
    "# Apply the ClassLabel to the 'label' column.\n",
    "# This converts the string labels (\"surfing\", \"noise\") into integer indices (0, 1)\n",
    "# which are required by the model for training.\n",
    "train_dataset = train_dataset.cast_column('label', class_label_feature)\n",
    "\n",
    "display(train_dataset)\n",
    "display(train_dataset.features)\n",
    "\n",
    "# Verify that the label has been converted to an integer\n",
    "print(f\"\\nSample label (as integer): {train_dataset[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81de1aef-a17a-4a77-bb09-7f9d095246ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the 'image' column contains raw bytes, as expected by our preprocess function\n",
    "type(train_dataset[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2679f13a-f956-4897-8d76-7aa6c9a60d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment Setup"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow.pytorch\n",
    "import mlflow.data\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# set device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\") # Will be 'cpu' in this environment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2089dcb2-c2d2-447b-853f-b000fb10afdc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model : Transfer learning"
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of labels from our ClassLabel feature\n",
    "num_labels = train_dataset.features['label'].num_classes\n",
    "print(f\"\\n{num_labels} classes found in features.\")\n",
    "\n",
    "# Load the pre-trained model's image processor\n",
    "# This processor knows how to resize (224x224), normalize, \n",
    "# and convert images to the exact format the ViT model expects.\n",
    "processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224', use_fast = True, do_resize = True, size = 224)\n",
    "\n",
    "# Load the pre-trained ViT model\n",
    "# 'ignore_mismatched_sizes = True' is essential.\n",
    "# It discards the original 1000-class ImageNet classifier head \n",
    "# and replaces it with a new, randomly initialized classifier \n",
    "# that matches our 'num_labels' (2).\n",
    "model = AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels = num_labels, ignore_mismatched_sizes = True)\n",
    "\n",
    "# Set label-to-ID mappings in the model's config for clarity\n",
    "model.config.id2label = {i: label for i, label in enumerate(class_label_feature.names)}\n",
    "model.config.label2id = {label: i for i, label in enumerate(class_label_feature.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381c32e5-07ab-48e3-b363-d38afeaf26ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Input Image"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    '''\n",
    "    define a preprocessing function to handle a single data sample (raw bytes image data)\n",
    "    input: example = {'name': '...', 'path': '...', 'label': 0, 'image': b'...' }\n",
    "    return: example = {'name': '...', 'path': '...', 'label': 0, 'image': b'...', 'pixel_values': tensor([3, 224, 224])} \n",
    "    '''\n",
    "    # 1. Decode the raw 'bytes' into a PIL Image object\n",
    "    # 2. Convert to 'RGB' (3 channels). This is critical, as it turns our\n",
    "    #    binary/grayscale images into the 3-channel format ViT expects.\n",
    "    image = Image.open(io.BytesIO(example['image'])).convert('RGB')\n",
    "\n",
    "    # 3. Use the processor to resize, normalize, and convert the PIL image \n",
    "    #    into a PyTorch tensor. Output shape is [1, 3, 224, 224].\n",
    "    processed_img = processor(images = image, return_tensors = 'pt')\n",
    "    \n",
    "    # 4. Remove the unnecessary batch dimension (axis 0). \n",
    "    #    Shape becomes [3, 224, 224]. The Trainer's data loader will re-add \n",
    "    #    the batch dimension later.\n",
    "    example['pixel_values'] = processed_img['pixel_values'].squeeze()\n",
    "    return example\n",
    "\n",
    "print(\"Starting preprocessing with .map() on Databricks...\")\n",
    "# Apply the preprocessing function to all samples in the dataset.\n",
    "# .map() caches the results for fast training.\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "print(\"Preprocessing finished and cached.\")\n",
    "\n",
    "# Check a sample to verify the new 'pixel_values' column\n",
    "sample = train_dataset[0]\n",
    "print(sample.keys())\n",
    "print(f\"Pixel values shape: {torch.tensor(sample['pixel_values']).shape}\")\n",
    "print(f\"Label: {sample['label']}\")\n",
    "\n",
    "# Set the dataset format to return PyTorch tensors directly\n",
    "train_dataset.set_format(type='torch', columns=['pixel_values', 'label'])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "# 'stratify_by_column' is crucial for imbalanced data.\n",
    "# It ensures both train_set and test_set have the same 80/20 ratio\n",
    "# of normal/noise labels as the original dataset.\n",
    "train_dataset = train_dataset.train_test_split(test_size = 0.2, stratify_by_column='label')\n",
    "train_set = train_dataset['train']\n",
    "test_set = train_dataset['test']\n",
    "\n",
    "# Log the class distribution\n",
    "train_normal_num = len(train_set['label'][train_set['label'] == 0])\n",
    "train_noise_num = len(train_set['label'][train_set['label'] == 1])\n",
    "print(f\"train_normal_num: {train_normal_num}, train_noise_num: {train_noise_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82c9679-1fc8-47f2-a93c-5fab37035fcb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Classifier Weights"
    }
   },
   "outputs": [],
   "source": [
    "# Although 'ignore_mismatched_sizes=True' already created a new head,\n",
    "# we can also explicitly re-initialize it for full control.\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)\n",
    "\n",
    "# Initialize the weights of the new classifier layer\n",
    "# 'xavier_uniform_' is a common and robust initialization strategy.\n",
    "torch.nn.init.xavier_uniform_(model.classifier.weight)\n",
    "model.classifier.bias.data.fill_(0)\n",
    "\n",
    "# We comment this out because the Trainer will handle it.\n",
    "# The Trainer automatically moves the model to the correct device ('cpu' or 'cuda')\n",
    "# when training begins, so a manual .to(device) is not needed.\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe69f41-6eaf-4e35-a89b-09753a215674",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training Setup"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'vit-base-patch16-224-anomaly'\n",
    "# Define the output directory on DBFS (Databricks File System) for persistent storage.\n",
    "# '/dbfs' is the FUSE mount point for DBFS.\n",
    "output_path = f\"/dbfs{mount_path}/{model_name}\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "  output_dir=output_path,   # Where to save checkpoints\n",
    "  num_train_epochs=10,\n",
    "  per_device_train_batch_size=16, # Num samples per batch (on this device)\n",
    "  # Simulate a larger batch size for stable gradients. \n",
    "  # Effective batch size = 16 * 4 = 64.\n",
    "  gradient_accumulation_steps=4,      \n",
    "  per_device_eval_batch_size=16,  # Batch size for evaluation      \n",
    "  learning_rate=2e-5,           # Standard learning rate for ViT fine-tuning\n",
    "  weight_decay=0.01,            # L2 regularization to prevent overfitting\n",
    "  # Use a learning rate warmup for 10% of training steps.\n",
    "  # This helps stabilize training at the beginning.\n",
    "  warmup_ratio=0.1, \n",
    "  evaluation_strategy=\"epoch\",  # Run evaluation at the end of each epoch\n",
    "  save_strategy=\"epoch\",        # Save a checkpoint at the end of each epoch\n",
    "  logging_strategy='steps',\n",
    "  logging_steps=10,             # Log training loss every 10 steps\n",
    "  # Our dataset is 80/20, so 'accuracy' is misleading.\n",
    "  # 'f1' score for the minority class is the correct metric to optimize for.\n",
    "  metric_for_best_model='f1',          \n",
    "  # This ensures that 'trainer.model' at the end is the best-performing one.\n",
    "  load_best_model_at_end=True,\n",
    "  # Only keep the single best checkpoint to save disk space.\n",
    "  save_total_limit=1,\n",
    "  \n",
    "  # We are training on CPU, so FP16 (mixed precision) must be disabled.\n",
    "  # FP16 is for NVIDIA GPUs.\n",
    "  fp16=False,                          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4193c499-6848-430f-ba26-57fd57b3ef24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation Metrics"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate # Hugging Face's modern evaluation library\n",
    "\n",
    "# Load the F1 and Accuracy metric calculators\n",
    "f1_metric= evaluate.load(\"f1\")\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculates F1 and Accuracy for the Trainer.\"\"\"\n",
    "    # eval_pred is a tuple: (predictions, label_ids)\n",
    "    # predictions are raw logits (e.g., [1.7, -0.3]), not probabilities\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    \n",
    "    # Get the predicted class (0 or 1) by finding the index \n",
    "    # with the highest logit score for each sample (axis=1).\n",
    "    predicts = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # --- Calculate Accuracy ---\n",
    "    acc_result = acc_metric.compute(predictions = predicts, references = labels)\n",
    "    \n",
    "    # --- Calculate F1 Score (for the minority class) ---\n",
    "    # 'pos_label=1' specifies that our 'positive' class (the one we \n",
    "    # want to detect) is 'noise' (label 1).\n",
    "    # 'average=\"binary\"' tells the function to *only* calculate the F1 \n",
    "    # score for this specific positive class (label 1).\n",
    "    f1_result = f1_metric.compute(predictions = predicts, references = labels, average = \"binary\", pos_label = 1)\n",
    "\n",
    "    # Combine both metrics into a single dictionary\n",
    "    results = {}\n",
    "    results.update(acc_result)\n",
    "    results.update(f1_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2aade4-7a18-41a2-865e-21cfb49773da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "train"
    }
   },
   "outputs": [],
   "source": [
    "run_name = f\"{model_name}-run-{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    # 1. Enable autologging, but disable model logging.\n",
    "    # We do this because autolog() was unreliably failing to save artifacts\n",
    "    # in this environment. We let it handle params/metrics,\n",
    "    # but we will manually log the model ourselves.\n",
    "    mlflow.transformers.autolog(log_models = False)\n",
    "\n",
    "    # 2. Log custom parameters that autolog cannot know\n",
    "    mlflow.log_param(\"train_normal_num\", train_normal_num)\n",
    "    mlflow.log_param(\"train_noise_num\", train_noise_num)\n",
    "    mlflow.log_param(\"total_train_samples\", len(train_set))\n",
    "    mlflow.log_param(\"total_test_samples\", len(test_set))\n",
    "\n",
    "    mlflow.set_tag(\"task\", \"anomaly_detection\")\n",
    "    mlflow.set_tag(\"model_family\", \"ViT\")\n",
    "\n",
    "    # 3. Log the input dataset for data lineage tracking.\n",
    "    # This creates a *reference* to the 'train_df' Spark table.\n",
    "    # This line *required* the Spark Auth setup from Cell 7 to work.\n",
    "    src_dataset = mlflow.data.from_spark(train_df)\n",
    "    mlflow.log_input(src_dataset, context=\"Training-Input-DataFrame\")\n",
    "\n",
    "    # 4. Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # 5. Start the model training\n",
    "    # autolog() will automatically log metrics/params during this process.\n",
    "    print(\"Starting model training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # 6. Manually log the model artifacts\n",
    "    # This block runs *after* training is complete.\n",
    "    print(\"Logging model artifacts manually...\")\n",
    "    try:\n",
    "        # Get a sample input for signature inference\n",
    "        sample_input = next(iter(test_set))\n",
    "        # Add the batch dimension: [3, 224, 224] -> [1, 3, 224, 224]\n",
    "        input_example = sample_input['pixel_values'].unsqueeze(0).cpu().numpy()\n",
    "        \n",
    "        # Log the model using mlflow.transformers.log_model\n",
    "        mlflow.transformers.log_model(\n",
    "            # CRITICAL: We must pass a dict containing *both* the model\n",
    "            # and the processor. 'trainer.model' contains the best model\n",
    "            # thanks to 'load_best_model_at_end=True'.\n",
    "            transformers_model = {\"model\": trainer.model, \"image_processor\": processor}, \n",
    "            artifact_path = \"model\", # Folder name in MLflow artifacts\n",
    "            input_example = input_example # This infers the signature automatically\n",
    "        )\n",
    "        print(\"Model artifacts logged successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to log model artifacts: {e}\")\n",
    "\n",
    "print(\"MLflow run completed!!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5698626561095429,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

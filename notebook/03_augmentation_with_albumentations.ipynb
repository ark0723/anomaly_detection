{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9220a00-ac29-4506-ad41-cbaafcd781da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"albumentations\" \"numpy<2.0\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fadfb9-8b61-49c0-9437-c6c5015f7493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f390f0b5-7cf0-47d2-9a15-a3a05c767a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_path = \"/mnt/datamount\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a8cf50-0836-4159-9d3c-870b57e184e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define Augmentation Pipelines\n",
    "TARGET_RESIZE = 256\n",
    "\n",
    "def custom_solarize(image, threshold=128, **kwargs):\n",
    "    \"\"\"Solarize: Invert only pixels brighter than the threshold.\"\"\"\n",
    "    # np.where(condition, value_if_true, value_if_false)\n",
    "    img_out = np.where(image > threshold, 255 - image, image)\n",
    "    return img_out.astype(np.uint8)\n",
    "\n",
    "def custom_invert_img(image, **kwargs):\n",
    "    \"\"\"InvertImg: Invert all pixels.\"\"\"\n",
    "    return (255 - image).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Strategy 1: Combination of weak transformations for 'normal' data\n",
    "normal_transform = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    A.Rotate(limit=10, p=0.7), # -10 to +10 degree rotation\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.7), # ~10% shift/scale\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.8), # Adjust brightness/contrast\n",
    "    A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.5),\n",
    "    # A.GaussianBlur(blur_limit=(3, 7), p=0.5), # Weak blur\n",
    "    # A.MotionBlur(blur_limit=(3, 9), p=0.5) \n",
    "])\n",
    "\n",
    "# Strategy 2A: Geometric transformations for existing 'anomaly' data\n",
    "anomaly_transform_A = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    A.Rotate(limit=15, p=0.8),\n",
    "    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=0, p=0.8),\n",
    "])\n",
    "\n",
    "# Strategy 2B: Strong transformations to turn 'normal' data into 'new anomalies'\n",
    "anomaly_transform_B = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    \n",
    "    # 1. Create 'blemishes' (OneOf ensures at least one is applied)\n",
    "    A.OneOf([\n",
    "        # 'Black spots/peeling' (patch-style)\n",
    "        A.CoarseDropout(\n",
    "            max_holes=8, max_height=32, max_width=32,\n",
    "            min_holes=1, min_height=16, min_width=16,\n",
    "            fill_value=0, p=0.8\n",
    "        ),\n",
    "        # 'Cracks' or 'long scratches'\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, p=0.7),\n",
    "        \n",
    "        # 'Stains' or 'sensor noise'\n",
    "        A.GaussNoise(var_limit=(100.0, 200.0), p=0.7),\n",
    "        \n",
    "        # 'Compression artifacts'\n",
    "        A.ImageCompression(quality_lower=40, quality_upper=70, p=0.5),\n",
    "\n",
    "        # --- Salt & Pepper ---\n",
    "        # 'Pepper' (black dots) simulation\n",
    "        A.PixelDropout(dropout_prob=0.03, p=0.7, fill_value=0),\n",
    "        \n",
    "        # 'Salt' (white dots) simulation\n",
    "        A.PixelDropout(dropout_prob=0.03, p=0.7, fill_value=255)\n",
    "\n",
    "    ], p=1.0),\n",
    "\n",
    "    # 2. Add 'disturbances' (optional)\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "        A.MotionBlur(blur_limit=(3, 9), p=0.5)\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # 3. Geometric transformation\n",
    "    A.Rotate(limit=10, p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define Augmentation Quantities (Goal: ~800 normal, ~200 anomaly)\n",
    "# (800 target - 18 original) / 18 original = 43.44... -> Round up to 44\n",
    "N_PER_NORMAL = 44 # 18 original normal -> 18(original) + 18*44(augmented) = 810 (normal)\n",
    "\n",
    "# 2 original anomalies -> ~50 (Track A)\n",
    "# (50 target - 2 original) / 2 original = 24 -> Round up to 25 (to get >= 50 including originals)\n",
    "N_PER_ANOMALY_A = 25 # 2 original anomalies -> 2(original) + 2*25(augmented) = 52 (anomaly Track A)\n",
    "\n",
    "# 18 original normals -> ~150 (Track B)\n",
    "# 150 target / 18 original = 8.33... -> Round up to 9\n",
    "N_PER_NEW_ANOMALY = 9 # 18 original normal -> 18*9(augmented) = 162 (anomaly Track B)\n",
    "\n",
    "# Total anomalies: 52 (Track A) + 162 (Track B) = 214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ccb1d8-3f7a-40cc-a587-a0ba06d18690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Image Transformation Helper Functions (PIL & Numpy)\n",
    "\n",
    "def bin_to_np(img_bin):\n",
    "    \"\"\"\n",
    "    Converts Spark BinaryType to a Numpy array (RGB).\n",
    "    - This is a very CPU-expensive operation: minimizing its execution count is key to efficiency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use PIL to open the binary data and convert to RGB\n",
    "        image = Image.open(io.BytesIO(img_bin)).convert('RGB')\n",
    "        # Convert to Numpy array\n",
    "        return np.array(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding image: {e}\")\n",
    "        return None\n",
    "\n",
    "def np_to_bin(img_np):\n",
    "    \"\"\"\n",
    "    Converts a Numpy array (RGB) back to Spark BinaryType (PNG).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Albumentations returns an RGB numpy array\n",
    "        # Image.fromarray accepts the RGB order directly\n",
    "        image = Image.fromarray(img_np)\n",
    "        \n",
    "        # Create an in-memory byte buffer\n",
    "        byte_io = io.BytesIO()\n",
    "        \n",
    "        # Save the image to the byte buffer in PNG format\n",
    "        # Using lossless compression (PNG) to preserve original features.\n",
    "        image.save(byte_io, format='PNG')\n",
    "        \n",
    "        # Return the full byte value from the buffer\n",
    "        return byte_io.getvalue()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode image with PIL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 4. Pandas UDF (Generator) Definitions\n",
    "\n",
    "# Define the schema that the UDFs will return\n",
    "output_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"image\", BinaryType(), False)\n",
    "])\n",
    "\n",
    "# Column order for the output DataFrame (must match the schema)\n",
    "SCHEMA_COLUMNS = [\"name\", \"path\", \"label\", \"image\"]\n",
    "\n",
    "# UDF 1: For processing 'normal' images\n",
    "def augment_normal_generator(pdf_iter):\n",
    "    \"\"\"\n",
    "    A Pandas UDF generator that takes an iterator of 'normal' image partitions.\n",
    "    For each normal image, it yields:\n",
    "    1. The original image.\n",
    "    2. N 'normal' augmentations (Strategy 1).\n",
    "    3. M 'new anomaly' augmentations (Strategy 2B).\n",
    "    \"\"\"\n",
    "    for pdf in pdf_iter:\n",
    "        # A list to store all rows that will be generated from this partition\n",
    "        output_rows = []\n",
    "\n",
    "        for _, row in pdf.iterrows():\n",
    "            name, path, label = row['name'], row['path'], row['label']\n",
    "            \n",
    "            img_np = bin_to_np(row['image'])\n",
    "            if img_np is None:\n",
    "                continue\n",
    "\n",
    "            # 1. Add the original image\n",
    "            output_rows.append((name, path, label, row['image']))\n",
    "\n",
    "            # 2. 'Normal' augmentation (Strategy 1)\n",
    "            for i in range(N_PER_NORMAL):\n",
    "                augmented_np = normal_transform(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_normal_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", label, aug_bin))\n",
    "\n",
    "            # 3. 'New Anomaly' generation (Strategy 2B)\n",
    "            for i in range(N_PER_NEW_ANOMALY):\n",
    "                augmented_np = anomaly_transform_B(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_new_anomaly_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", \"noise\", aug_bin))\n",
    "                    \n",
    "        # After processing all rows in the partition, yield the list as a DataFrame\n",
    "        if output_rows:\n",
    "            yield pd.DataFrame(output_rows, columns=SCHEMA_COLUMNS)\n",
    "\n",
    "# UDF 2: For processing 'anomaly' images\n",
    "def augment_anomaly_generator(pdf_iter):\n",
    "    \"\"\"\n",
    "    A Pandas UDF generator that takes an iterator of 'anomaly' image partitions.\n",
    "    'pdf_iter' is an iterator that yields Spark partitions (data chunks) one by one.\n",
    "    'pdf' is a single partition, represented as a pandas DataFrame.\n",
    "    \n",
    "    For each anomaly image, it yields:\n",
    "    1. The original anomaly image.\n",
    "    2. K 'anomaly' augmentations (Strategy 2A).\n",
    "    \"\"\"\n",
    "    for pdf in pdf_iter:\n",
    "        # A list to store all rows that will be generated from this partition\n",
    "        output_rows = []\n",
    "\n",
    "        for _, row in pdf.iterrows():\n",
    "            name, path, label = row['name'], row['path'], row['label']\n",
    "            \n",
    "            img_np = bin_to_np(row['image'])\n",
    "            if img_np is None:\n",
    "                continue\n",
    "\n",
    "            # 1. Return the original anomaly image\n",
    "            output_rows.append((name, path, label, row['image']))\n",
    "\n",
    "            # 2. 'Anomaly' augmentation (Strategy 2A)\n",
    "            for i in range(N_PER_ANOMALY_A):\n",
    "                augmented_np = anomaly_transform_A(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_anomaly_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", label, aug_bin))\n",
    "                    \n",
    "        # After processing all rows in the partition, yield the list as a DataFrame\n",
    "        if output_rows:\n",
    "            yield pd.DataFrame(output_rows, columns=SCHEMA_COLUMNS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2d202a-86d6-4c19-b5e6-dbdf29a78016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Execute Spark Job\n",
    "print(\"Loading original DataFrame...\")\n",
    "df = spark.read.format('parquet').load(f'{mount_path}/images_final')\n",
    "\n",
    "\n",
    "# Separate data and cache\n",
    "df_normal = df.filter(F.col('label') == 'surfing').cache()\n",
    "df_anomaly = df.filter(F.col('label') == 'noise').cache()\n",
    "\n",
    "print(f\"Original Normal Count: {df_normal.count()}, Original Anomaly Count: {df_anomaly.count()}\")\n",
    "\n",
    "print(\"Starting Augmentation (UDF 1: Normal -> Normal + New Anomaly)...\")\n",
    "df_aug_1 = df_normal.mapInPandas(augment_normal_generator, schema=output_schema)\n",
    "\n",
    "print(\"Starting Augmentation (UDF 2: Anomaly -> Anomaly)...\")\n",
    "df_aug_2 = df_anomaly.mapInPandas(augment_anomaly_generator, schema=output_schema)\n",
    "\n",
    "# Combine the two results\n",
    "df_augmented = df_aug_1.unionByName(df_aug_2)\n",
    "\n",
    "# Check the results\n",
    "print(\"Augmentation complete. Final result aggregation:\")\n",
    "df_augmented.groupBy('label').count().show()\n",
    "\n",
    "# (Optional) Unpersist caches\n",
    "df_normal.unpersist()\n",
    "df_anomaly.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4152be90-eb3d-4ea1-b496-11edffd67f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define image metadata\n",
    "img_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\":\"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "df_augmented = df_augmented.withColumn('image', F.col('image').alias('image', metadata = img_meta))\n",
    "display(df_augmented, limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34137665-d379-4586-99b0-42ea77c0528f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_augmented.write.mode(\"overwrite\").format('parquet').save(f'{mount_path}/images_augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac810e8-8df7-4398-9970-dc02d359c26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_augmentation_with_albumentations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

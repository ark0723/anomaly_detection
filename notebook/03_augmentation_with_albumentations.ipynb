{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9220a00-ac29-4506-ad41-cbaafcd781da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"albumentations\" \"numpy<2.0\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fadfb9-8b61-49c0-9437-c6c5015f7493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f390f0b5-7cf0-47d2-9a15-a3a05c767a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_path = \"/mnt/datamount\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a8cf50-0836-4159-9d3c-870b57e184e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Augmentation 파이프라인 정의\n",
    "TARGET_RESIZE = 256\n",
    "\n",
    "def custom_solarize(image, threshold=128, **kwargs):\n",
    "    \"\"\"Solarize: 임계값보다 밝은 픽셀만 반전\"\"\"\n",
    "    # np.where(조건, True일 때 값, False일 때 값)\n",
    "    img_out = np.where(image > threshold, 255 - image, image)\n",
    "    return img_out.astype(np.uint8)\n",
    "\n",
    "def custom_invert_img(image, **kwargs):\n",
    "    \"\"\"InvertImg: 전체 픽셀 반전\"\"\"\n",
    "    return (255 - image).astype(np.uint8)\n",
    "\n",
    "\n",
    "# 전략 1: '정상' 데이터를 위한 약한 변형 조합\n",
    "normal_transform = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    A.Rotate(limit=10, p=0.7), # -10~+10도 회전\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.7), # 10% 내외 이동/확대\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.8), # 밝기/대비 조절\n",
    "    A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.5),\n",
    "    # A.GaussianBlur(blur_limit=(3, 7), p=0.5), # 약한 블러\n",
    "    # A.MotionBlur(blur_limit=(3, 9), p=0.5) \n",
    "])\n",
    "\n",
    "# 전략 2A: 기존 '비정상' 데이터를 위한 기하학적 변형\n",
    "anomaly_transform_A = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    A.Rotate(limit=15, p=0.8),\n",
    "    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=0, p=0.8),\n",
    "])\n",
    "\n",
    "# 전략 2B: '정상' 데이터를 '새로운 비정상'으로 만들기 위한 강한 변형\n",
    "anomaly_transform_B = A.Compose([\n",
    "    A.Resize(TARGET_RESIZE, TARGET_RESIZE),\n",
    "    \n",
    "    # 1. '오점' 생성 (OneOf로 하나 이상 반드시 적용)\n",
    "    A.OneOf([\n",
    "        # '검은 점/벗겨짐' (패치형)\n",
    "        A.CoarseDropout(\n",
    "            max_holes=8, max_height=32, max_width=32,\n",
    "            min_holes=1, min_height=16, min_width=16,\n",
    "            fill_value=0, p=0.8\n",
    "        ),\n",
    "        # '크랙(crack)'이나 '긴 스크래치'\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, p=0.7),\n",
    "        \n",
    "        # '얼룩'이나 '센서 노이즈'\n",
    "        A.GaussNoise(var_limit=(100.0, 200.0), p=0.7),\n",
    "        \n",
    "        # '압축 깨짐'\n",
    "        A.ImageCompression(quality_lower=40, quality_upper=70, p=0.5),\n",
    "\n",
    "        # --- Salt & Pepper ---\n",
    "        # 'Pepper' (검은색 점) 시뮬레이션\n",
    "        A.PixelDropout(dropout_prob=0.03, p=0.7, fill_value=0),\n",
    "        \n",
    "        # 'Salt' (흰색 점) 시뮬레이션\n",
    "        A.PixelDropout(dropout_prob=0.03, p=0.7, fill_value=255)\n",
    "\n",
    "    ], p=1.0),\n",
    "\n",
    "    # 2. '방해 요소' 추가 (선택적)\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "        A.MotionBlur(blur_limit=(3, 9), p=0.5)\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # 3. 기하학적 변형\n",
    "    A.Rotate(limit=10, p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Augmentation 수량 정의 (목표: 정상 ~800, 비정상 ~200)\n",
    "# (800 - 18) / 18 = 43.44... -> 44로 올림\n",
    "N_PER_NORMAL = 44 # 원본 정상(18) -> 18(원본) + 18*44(증강) = 810 (정상)\n",
    "\n",
    "# 원본 비정상 2개 -> ~50개 (Track A)\n",
    "# (50 - 2) / 2 = 24 -> 25로 올림 (원본 포함 50개 이상)\n",
    "N_PER_ANOMALY_A = 25 # 원본 비정상(2) -> 2(원본) + 2*25(증강) = 52 (비정상 Track A)\n",
    "\n",
    "# 원본 정상 18개 -> ~150개 (Track B)\n",
    "# 150 / 18 = 8.33... -> 9로 올림\n",
    "N_PER_NEW_ANOMALY = 9 # 원본 정상(18) -> 18*9(증강) = 162 (비정상 Track B)\n",
    "\n",
    "# 총 비정상: 52 (Track A) + 162 (Track B) = 214개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ccb1d8-3f7a-40cc-a587-a0ba06d18690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. 이미지 변환 헬퍼 함수 (PIL & Numpy)\n",
    "\n",
    "def bin_to_np(img_bin):\n",
    "    \"\"\"Spark BinaryType을 Numpy 배열(RGB)로 변환\"\"\"\n",
    "    try:\n",
    "        # PIL을 사용해 바이너리를 열고 RGB로 변환\n",
    "        image = Image.open(io.BytesIO(img_bin)).convert('RGB')\n",
    "        # Numpy 배열로 변환\n",
    "        return np.array(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding image: {e}\")\n",
    "        return None\n",
    "\n",
    "def np_to_bin(img_np):\n",
    "    \"\"\"\n",
    "    Numpy 배열(RGB)을 Spark BinaryType(JPEG)으로 변환\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Albumentations는 RGB numpy 배열을 반환\n",
    "        # Image.fromarray는 RGB 순서를 그대로 받음\n",
    "        image = Image.fromarray(img_np)\n",
    "        \n",
    "        # 메모리 내의 바이트 버퍼 생성\n",
    "        byte_io = io.BytesIO()\n",
    "        \n",
    "        # 이미지를 PNG 포맷으로 바이트 버퍼에 저장\n",
    "        # 원본 특성 유지를 위해 무손실 압축(PNG).\n",
    "        image.save(byte_io, format='PNG')\n",
    "        \n",
    "        # 버퍼의 전체 바이트 값을 반환\n",
    "        return byte_io.getvalue()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode image with PIL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 4. Pandas UDF (Generator) 정의\n",
    "\n",
    "# UDF가 반환할 스키마 정의\n",
    "output_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"image\", BinaryType(), False)\n",
    "])\n",
    "\n",
    "# UDF가 반환할 DataFrame의 컬럼 순서 (스키마와 일치해야 함)\n",
    "SCHEMA_COLUMNS = [\"name\", \"path\", \"label\", \"image\"]\n",
    "\n",
    "# UDF 1: 정상 이미지 처리용\n",
    "def augment_normal_generator(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        # 이 파티션에서 생성될 모든 행을 저장할 리스트\n",
    "        output_rows = []\n",
    "\n",
    "        for _, row in pdf.iterrows():\n",
    "            name, path, label = row['name'], row['path'], row['label']\n",
    "            \n",
    "            img_np = bin_to_np(row['image'])\n",
    "            if img_np is None:\n",
    "                continue\n",
    "\n",
    "            # 1. 원본 이미지 추가\n",
    "            output_rows.append((name, path, label, row['image']))\n",
    "\n",
    "            # 2. '정상' 증강 (Strategy 1)\n",
    "            for i in range(N_PER_NORMAL):\n",
    "                augmented_np = normal_transform(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_normal_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", label, aug_bin))\n",
    "\n",
    "            # 3. '새로운 비정상' 생성 (Strategy 2B)\n",
    "            for i in range(N_PER_NEW_ANOMALY):\n",
    "                augmented_np = anomaly_transform_B(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_new_anomaly_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", \"noise\", aug_bin))\n",
    "        # 파티션의 모든 행 처리가 끝나면, 리스트를 DataFrame으로 변환하여 yield\n",
    "        if output_rows:\n",
    "            yield pd.DataFrame(output_rows, columns=SCHEMA_COLUMNS)\n",
    "\n",
    "# UDF 2: 비정상 이미지 처리용\n",
    "def augment_anomaly_generator(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        # 이 파티션에서 생성될 모든 행을 저장할 리스트\n",
    "        output_rows = []\n",
    "\n",
    "        for _, row in pdf.iterrows():\n",
    "            name, path, label = row['name'], row['path'], row['label']\n",
    "            \n",
    "            img_np = bin_to_np(row['image'])\n",
    "            if img_np is None:\n",
    "                continue\n",
    "\n",
    "            # 1. 원본 비정상 이미지 반환\n",
    "            output_rows.append((name, path, label, row['image']))\n",
    "\n",
    "            # 2. '비정상' 증강 (Strategy 2A)\n",
    "            for i in range(N_PER_ANOMALY_A):\n",
    "                augmented_np = anomaly_transform_A(image=img_np)['image']\n",
    "                aug_bin = np_to_bin(augmented_np)\n",
    "                if aug_bin:\n",
    "                    new_name = f\"{name}_aug_anomaly_{i}\"\n",
    "                    output_rows.append((new_name, \"N/A\", label, aug_bin))\n",
    "        # 파티션의 모든 행 처리가 끝나면, 리스트를 DataFrame으로 변환하여 yield\n",
    "        if output_rows:\n",
    "            yield pd.DataFrame(output_rows, columns=SCHEMA_COLUMNS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2d202a-86d6-4c19-b5e6-dbdf29a78016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Spark 작업 실행\n",
    "print(\"원본 DataFrame 로드 중...\")\n",
    "df = spark.read.format('parquet').load(f'{mount_path}/images_final')\n",
    "\n",
    "\n",
    "# 데이터 분리 및 캐시\n",
    "df_normal = df.filter(F.col('label') == 'surfing').cache()\n",
    "df_anomaly = df.filter(F.col('label') == 'noise').cache()\n",
    "\n",
    "print(f\"정상 원본: {df_normal.count()}, 비정상 원본: {df_anomaly.count()}\")\n",
    "\n",
    "print(\"Augmentation 시작 (UDF 1: Normal -> Normal + New Anomaly)...\")\n",
    "df_aug_1 = df_normal.mapInPandas(augment_normal_generator, schema=output_schema)\n",
    "\n",
    "print(\"Augmentation 시작 (UDF 2: Anomaly -> Anomaly)...\")\n",
    "df_aug_2 = df_anomaly.mapInPandas(augment_anomaly_generator, schema=output_schema)\n",
    "\n",
    "# 두 결과 합치기\n",
    "df_augmented = df_aug_1.unionByName(df_aug_2)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Augmentation 완료. 최종 결과 집계:\")\n",
    "df_augmented.groupBy('label').count().show()\n",
    "\n",
    "# (선택 사항) 캐시 해제\n",
    "df_normal.unpersist()\n",
    "df_anomaly.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4152be90-eb3d-4ea1-b496-11edffd67f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define image metadata\n",
    "img_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\":\"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "df_augmented = df_augmented.withColumn('image', F.col('image').alias('image', metadata = img_meta))\n",
    "display(df_augmented, limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34137665-d379-4586-99b0-42ea77c0528f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_augmented.write.mode(\"overwrite\").format('parquet').save(f'{mount_path}/images_augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac810e8-8df7-4398-9970-dc02d359c26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_augmentation_with_albumentations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5153a52-7c09-4741-b302-2fb23013ef25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_path = \"/mnt/datamount\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd326af-e878-4c3f-852b-47ba653b23ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9a9712-07e7-41b6-9908-42348136d6f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa3b689-f3f9-4b4a-817b-4559b3b21492",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Images"
    }
   },
   "outputs": [],
   "source": [
    "def create_file_info_df(imgs_dir: str):\n",
    "    full_path = f\"{mount_path}/{imgs_dir}\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(full_path)\n",
    "    except Exception:\n",
    "        raise Exception(f\"Directory {full_path} does not exist\")\n",
    "        return None\n",
    "\n",
    "    file_info_list = [{\"path\": file.path, \"name\": file.name} for file in files]\n",
    "    return spark.createDataFrame(file_info_list)\n",
    "\n",
    "file_info_df = create_file_info_df(\"images\")\n",
    "\n",
    "# file_info_df: remove extension from 'name' column\n",
    "file_info_df = file_info_df.withColumn(\n",
    "    \"name\",\n",
    "    regexp_replace(\"name\", \"\\\\.[^.]+$\", \"\")\n",
    ")\n",
    "display(file_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2f2b8a-270e-49ce-a0fe-332f5a7f2f98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Labels"
    }
   },
   "outputs": [],
   "source": [
    "label_files = dbutils.fs.ls(f\"{mount_path}/labels\") # list of all files in the directory\n",
    "label_path = label_files[0].path\n",
    "label_df = spark.read.csv(label_path, header = True, inferSchema = True)\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7144e05f-cd48-44c0-bbf3-99057ad94e9f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761279739783}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "label_df = label_df.withColumnRenamed(\"img_name\", \"name\")\n",
    "label_df = label_df.withColumnRenamed(\"sports_activity\", \"label\")\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b4951c-17f0-42b9-8ab2-21a3d6785f38",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761279799141}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Join df: images and labels"
    }
   },
   "outputs": [],
   "source": [
    "# join the two dataframes with the 'name' column\n",
    "joined_df = file_info_df.join(\n",
    "    label_df,\n",
    "    on = \"name\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ce4712-b2b4-4a69-a0dd-3a6265f94d4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Image Display"
    }
   },
   "outputs": [],
   "source": [
    "def display_image(df, sample_size: int = 5, keep_order = True):\n",
    "    \"\"\"\n",
    "    Display sample images from a Spark DataFrame containing image file paths.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame with at least 'path' and 'name' columns.\n",
    "        sample_size (int): Number of images to display.\n",
    "        keep_order (bool): If True, display the first N images in order.\n",
    "                           If False, display N random images.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays images using matplotlib.\n",
    "    \"\"\"\n",
    "    if keep_order:\n",
    "        images = df.take(sample_size) # Get the first N rows as a list.\n",
    "    else:\n",
    "        # Get N random rows as a list.\n",
    "        # df.orderBy(F.rand()).limit(sample_size) : dataframe -> not callable\n",
    "        images = df.orderBy(F.rand()).limit(sample_size).collect()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, row in enumerate(images):\n",
    "        # Replace \"dbfs:/\" with \"/dbfs/\" for local file access in Databricks.\n",
    "        # why? FileNotFoundError: [Errno 2] No such file or directory: '/Workspace/Users/ark0723@gmail.com/anomaly_detection/notebook/dbfs:/mnt/datamount/images/frame_0.jpg' \n",
    "        img_path = row.path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1, sample_size, i+1)\n",
    "        plt.title(row.name)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960f6056-44c7-4705-99e9-d128821c0b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_image(joined_df, sample_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfddeaeb-b540-4aea-bcd2-aba4225a5c3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Crop & Resize"
    }
   },
   "outputs": [],
   "source": [
    "for row in joined_df.collect():\n",
    "    img_dir = row.path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    img = Image.open(img_dir)\n",
    "\n",
    "    w, h = img.size\n",
    "    new_size = min(w, h)\n",
    "    # center crop (x1, y1, x2, y2)\n",
    "    img = img.crop(((w-new_size)//2, (h-new_size)//2, (w+new_size)//2, (h+new_size)//2))\n",
    "    print(f\"{img.size = }\")\n",
    "\n",
    "    # resize : 256 x 256, if image enlarged, nearest neighbor will be applied)\n",
    "    img = img.resize((256, 256), Image.NEAREST)\n",
    "    print(f\"{img.size = }\")\n",
    "    plt.imshow(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a506233b-2662-4553-887e-b6e132d2e26d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pandas UDF"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, col, regexp_replace, lit\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# return 값은 BinaryType\n",
    "@pandas_udf(BinaryType())\n",
    "def resize_image_udf(img_paths: pd.Series) -> pd.Series:\n",
    "    def preprocess(path):\n",
    "        \"resize image and serialize back as jpg\"\n",
    "        # load image\n",
    "        img = Image.open(path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "        w, h = img.size\n",
    "        new_size = min(w, h)\n",
    "        # crop\n",
    "        img = img.crop(((w-new_size)//2, (h-new_size)//2, (w+new_size)//2, (h+new_size)//2))\n",
    "        # resize\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
    "        output = io.BytesIO()\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "    return img_paths.apply(preprocess)\n",
    "\n",
    "# add the metadata to enable the image preview\n",
    "img_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\":\"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "df = (\n",
    "    joined_df.withColumn(\"image\", resize_image_udf(\"path\")) # make 'image' column with binary image\n",
    "    .withColumn('image', col('image').alias('image', metadata = img_meta)) # binary image -> thumbnail(preview)\n",
    "    )\n",
    "\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721a4d8b-a816-4f2f-9d60-794dac080b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save df as parquet file in datalake storage\n",
    "df.write.mode(\"overwrite\").parquet(f\"{mount_path}/resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04de4918-0ed1-49d5-9ae9-2c5f8e79b580",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Augmentation"
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(BinaryType())\n",
    "def flip_image_horizontally_udf(df_series: pd.Series) -> pd.Series:\n",
    "    def flip(binary_image):\n",
    "        img = Image.open(io.BytesIO(binary_image))\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        output = io.BytesIO()\n",
    "        # save back as jpeg\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "    return df_series.apply(flip)\n",
    "\n",
    "df_flipped = df.withColumn(\"image\", flip_image_horizontally_udf(\"image\").alias(\"image\", metadata = img_meta)).withColumn(\"name\", regexp_replace(\"name\", r\"(\\d+)\", r\"$1_flipped\")).withColumn(\"path\", lit('N/A'))\n",
    "display(df_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8babd9c-6ebb-4f5a-ae3e-1db66f15c93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flipped.write.mode(\"append\").parquet(f\"{mount_path}/resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dd509c-35b3-4724-8438-cd7c2d04db63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.read.format('parquet').load(f'{mount_path}/resized')\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9baa5e85-909f-4fe5-9839-4019197c6139",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761604062446}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "add noise"
    }
   },
   "outputs": [],
   "source": [
    "noise_df = create_file_info_df(\"noisy_images\")\n",
    "noise_df = noise_df.withColumn(\"label\", lit(\"noise\"))\n",
    "noise_df = noise_df.withColumn(\"image\", resize_image_udf(\"path\")).withColumn('image', col('image').alias('image', metadata = img_meta))\n",
    "display(noise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c9c850-27f6-4b9f-b0cd-e8dcedcbf753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flipped_noise_df = noise_df.withColumn(\"image\", flip_image_horizontally_udf(\"image\").alias(\"image\", metadata = img_meta)).withColumn(\"name\", regexp_replace(\"name\", r\"(\\d+)\", r\"$1_noisy_flipped\")).withColumn(\"path\", lit('N/A'))\n",
    "display(flipped_noise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11efa824-0eb4-4a47-9e78-5dd0ed498f2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reduce instead of Union"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import DataFrame\n",
    "\n",
    "# final_df = new_df.union(noise_df).union(flipped_noise_df)\n",
    "final_df = reduce(DataFrame.unionAll, [new_df, noise_df, flipped_noise_df])\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f81b034-ac05-473d-9c1c-e0e494ce169f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save processed images"
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(f\"{mount_path}/images_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0f3839-625f-44b0-b1f3-3a68863d310a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df.groupBy('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9872d5d2-ac58-423b-ac80-39a1aacef633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_load_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

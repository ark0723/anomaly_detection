{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8948e368-e9b3-442c-8a3a-453f1b4a3836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_path = \"/mnt/datamount\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2a7367-6bea-47f0-83a1-f450299fcac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/ark0723@gmail.com/anomaly_detection/notebook/00_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6d1680-61ce-486f-8269-b7487b77d87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Image Augmentation\n",
    "- problem: class imblance between 'abnomal' and 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68879ba3-681b-4aac-b234-9d2805cfd724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load(f'{mount_path}/images_final')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad6a9fb-c42f-410a-a3fa-0bcbfbeb3bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.groupBy('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a72dc2-f18a-4e55-a2be-3e77061c0b36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEST: Augment Anomalies"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf, col, lit, regexp_replace\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import DataFrame\n",
    "\n",
    "\n",
    "def create_transform_udf(transforms: list):\n",
    "    \"\"\"\n",
    "    적용할 변환(transform) 타입 리스트를 인자로 받아,\n",
    "    해당 리스트에서 무작위로 변환을 선택하여 적용하는 Pandas UDF를 생성합니다.\n",
    "    (Geometric transforms + Salt&Pepper Noise)\n",
    "    \"\"\"\n",
    "    @pandas_udf(BinaryType())\n",
    "    def dynamic_transform_udf(df_series):\n",
    "        def apply_transform(img_bytes):\n",
    "            '''Apply transform to images and serialize back as jpeg.'''\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "            # randomly select transform\n",
    "            num_to_apply = random.randint(1, len(transforms))\n",
    "            seleted_list = random.sample(transforms, num_to_apply)\n",
    "\n",
    "            for selected_tr in seleted_list:\n",
    "                # *** 중요 ***\n",
    "                # 변환(예: 회전)으로 인해 이미지 크기가 바뀔 수 있으므로,\n",
    "                # 매번 루프가 돌 때마다 현재 크기를 다시 가져옵니다.\n",
    "                w, h = img.size\n",
    "\n",
    "                match selected_tr:\n",
    "                    case 'horizontal':\n",
    "                        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                    case 'vertical':\n",
    "                        img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                    case 'rotate_90':\n",
    "                        img = img.transpose(Image.ROTATE_90)    \n",
    "                    case 'rotate_180':\n",
    "                        img = img.transpose(Image.ROTATE_180)   \n",
    "                    case 'rotate_270':\n",
    "                        img = img.transpose(Image.ROTATE_270)  \n",
    "                    case 'squash&skew':\n",
    "                        # 현재 w, h를 기준으로 매트릭스 계산\n",
    "                        ss_matrix = (1, 0.3, -w*0.15, 0.3, 1, -h*0.15)\n",
    "                        img = img.transform((w,h), Image.AFFINE, ss_matrix)\n",
    "                    case 'salt&pepper':\n",
    "                        # draw 객체는 'img'를 직접 수정\n",
    "                        draw = ImageDraw.Draw(img)\n",
    "                        num_patches = np.random.randint(1, 5)\n",
    "\n",
    "                        for _ in range(num_patches):\n",
    "                            patch_pixels = np.random.randint(100, 500)\n",
    "                            noise_value = np.random.choice([0, 255]) # white or black\n",
    "                            # radius\n",
    "                            r = int(np.sqrt(patch_pixels / np.pi))\n",
    "                            r = max(r, 5) \n",
    "\n",
    "                            # 현재 w, h 기준으로 패치 생성 (이미지가 너무 작으면 skip)\n",
    "                            if w <=2*r or h<=2*r:\n",
    "                                continue\n",
    "\n",
    "                            cx = np.random.randint(r, w -r)\n",
    "                            cy = np.random.randint(r, h -r)\n",
    "\n",
    "                            # polygon\n",
    "                            num_points = np.random.randint(5,10) \n",
    "                            angles = np.linspace(0, 2*np.pi, num_points, endpoint=False)\n",
    "                            angles += np.random.uniform(0, 2*np.pi/num_points, size = num_points)\n",
    "                            radii = np.random.uniform(0.5*r, 1.5*r, size = num_points)\n",
    "                            points = [\n",
    "                                (int(cx + rad*np.cos(angle)), int(cy + rad*np.sin(angle)))\n",
    "                                for angle, rad in zip(angles, radii)\n",
    "                            ]\n",
    "\n",
    "                            fill_color = (noise_value,)*3 if img.mode == 'RGB' else noise_value\n",
    "                            draw.polygon(points, fill_color)\n",
    "            # save back as jpeg\n",
    "            output = io.BytesIO()\n",
    "            img.save(output, format='JPEG')\n",
    "            return output.getvalue()\n",
    "        return df_series.apply(apply_transform)\n",
    "    # return udf function\n",
    "    return dynamic_transform_udf\n",
    "\n",
    "# define image metadata\n",
    "img_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\":\"image/jpeg\"}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89765483-1f79-4d4d-9629-1711ed568a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. 적용할 모든 변환 유형을 리스트로 정의\n",
    "transform_to_anomaly = [\n",
    "    'horizontal', \n",
    "    'vertical', \n",
    "    'rotate_90', \n",
    "    'rotate_180', \n",
    "    'rotate_270', \n",
    "    'squash&skew',\n",
    "    'salt&pepper'\n",
    "]\n",
    "\n",
    "transform_to_normal = ['horizontal', 'vertical', 'rotate_90', 'rotate_180', 'rotate_270']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40fb01e6-2256-45ed-b605-f7a6f14d6c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### refactor: optimize spark performance\n",
    "- Filter Once: call df.filter(...) only one time, not in a loop.\n",
    "- posexplode: This is the standard Spark function to multiply rows. It takes an array and creates a new row for each element, giving you an index (named pos) for free.\n",
    "- Single Transformation: The UDF is applied to the entire set of duplicated rows at once. Spark handles the parallel execution across the whole cluster efficiently.\n",
    "- Unique Naming: Using the pos column from posexplode allows you to easily create a unique name for every new image.\n",
    "- One Write Operation: You trigger one single, large, and efficient Spark job, not hundreds of small ones. This is significantly faster and easier for Spark to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941236a8-2cef-4463-bab6-f043c4b7386f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_to_augment = 'surfing'\n",
    "target_normal = 800\n",
    "\n",
    "# 3. Get the count of base images to augment\n",
    "num_normal = df.filter(col('label') == label_to_augment).count()\n",
    "\n",
    "print(f\"Found {num_normal} base images with label '{label_to_augment}'.\")\n",
    "\n",
    "# 4. Calculate augmentations needed and handle edge cases\n",
    "if num_normal == 0:\n",
    "    print(\"No base images found. Skipping augmentation.\")\n",
    "else:\n",
    "    # Calculate how many new images to generate for *each* base image\n",
    "    n_augmentations_per_image = int(target_normal / num_normal) + 1 \n",
    "    \n",
    "    if n_augmentations_per_image == 0:\n",
    "        print(f\"Target ({target_normal}) is not greater than current count ({num_normal}). No augmentation needed.\")\n",
    "    else:\n",
    "        print(f\"Generating {n_augmentations_per_image} augmentations per image...\")\n",
    "\n",
    "        # 5. Create the UDF instance\n",
    "        normal_transformer_udf = create_transform_udf(transform_to_normal)\n",
    "\n",
    "        # 6. Define the Spark-native transformation\n",
    "        \n",
    "        # Filter the DataFrame *once*\n",
    "        base_df = df.filter(col('label') == label_to_augment)\n",
    "\n",
    "        # Create an array of N dummy elements, where N is the number of augmentations.\n",
    "        # This will be used to duplicate each row N times.\n",
    "        explode_array = F.array([F.lit(1)] * n_augmentations_per_image)\n",
    "\n",
    "        # Define the full transformation\n",
    "        augmented_df = base_df \\\n",
    "            .withColumn(\"explode_col\", explode_array) \\\n",
    "            .select(\n",
    "                \"*\", \n",
    "                # --- CHANGED LINE ---\n",
    "                # Call posexplode in its own 'select' clause.\n",
    "                # This creates two new columns: 'pos' (the index) and 'val' (the value)\n",
    "                F.posexplode(\"explode_col\").alias(\"pos\", \"val\") \n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"image\",\n",
    "                # Apply the random UDF. It will run once for each exploded row.\n",
    "                normal_transformer_udf(\"image\").alias(\"image\", metadata=img_meta)\n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"name\",\n",
    "                # Create a new unique name using the augmentation index ('pos')\n",
    "                F.concat(\n",
    "                    F.col(\"name\"), \n",
    "                    F.lit(\"_augmented_\"), \n",
    "                    F.col(\"pos\").cast(\"string\")\n",
    "                )\n",
    "            ) \\\n",
    "            .withColumn(\"path\", F.lit(\"N/A\")) \\\n",
    "            .select(\"name\", \"path\", \"label\", \"image\") # Re-order and drop temp columns\n",
    "\n",
    "# 7. Execute the *single* write operation\n",
    "\n",
    "# This one write operation executes the entire plan (filter, explode, transform)\n",
    "# in one optimized Spark job.\n",
    "augmented_df.write.mode('overwrite').format('parquet').save(f'{mount_path}/images_normal_final')\n",
    "\n",
    "print(f\"Successfully generated and saved {n_augmentations_per_image * num_normal} augmented images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bff7c1-bf0c-4d71-86f1-d38d54f05b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(augmented_df, limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69a5750-b053-4a8e-9e31-e225d52120bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Augment Anomalies"
    }
   },
   "outputs": [],
   "source": [
    "label_to_augment = 'noise'\n",
    "target_anormaly = 200\n",
    "\n",
    "# 3. Get the count of base images to augment\n",
    "num_anormaly = df.filter(col('label') == label_to_augment).count()\n",
    "\n",
    "print(f\"Found {num_anormaly} base images with label '{label_to_augment}'.\")\n",
    "\n",
    "# 4. Calculate augmentations needed and handle edge cases\n",
    "if num_anormaly == 0:\n",
    "    print(\"No base images found. Skipping augmentation.\")\n",
    "else:\n",
    "    # Calculate how many new images to generate for *each* base image\n",
    "    n_augmentations_per_image = int(target_anormaly / num_anomaly) + 1 \n",
    "    \n",
    "    if n_augmentations_per_image == 0:\n",
    "        print(f\"Target ({target_anormaly}) is not greater than current count ({num_anormaly}). No augmentation needed.\")\n",
    "    else:\n",
    "        print(f\"Generating {n_augmentations_per_image} augmentations per image...\")\n",
    "\n",
    "        # 5. Create the UDF instance\n",
    "        abnormal_transformer_udf = create_transform_udf(transform_to_anomaly)\n",
    "\n",
    "        # 6. Define the Spark-native transformation\n",
    "        \n",
    "        # Filter the DataFrame *once*\n",
    "        base_df = df.filter(col('label') == label_to_augment)\n",
    "\n",
    "        # Create an array of N dummy elements, where N is the number of augmentations.\n",
    "        # This will be used to duplicate each row N times.\n",
    "        explode_array = F.array([F.lit(1)] * n_augmentations_per_image)\n",
    "\n",
    "        # Define the full transformation\n",
    "        augmented_df = base_df \\\n",
    "            .withColumn(\"explode_col\", explode_array) \\\n",
    "            .select(\n",
    "                \"*\", \n",
    "                # --- CHANGED LINE ---\n",
    "                # Call posexplode in its own 'select' clause.\n",
    "                # This creates two new columns: 'pos' (the index) and 'val' (the value)\n",
    "                F.posexplode(\"explode_col\").alias(\"pos\", \"val\") \n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"image\",\n",
    "                # Apply the random UDF. It will run once for each exploded row.\n",
    "                abnormal_transformer_udf(\"image\").alias(\"image\", metadata=img_meta)\n",
    "            ) \\\n",
    "            .withColumn(\n",
    "                \"name\",\n",
    "                # Create a new unique name using the augmentation index ('pos')\n",
    "                F.concat(\n",
    "                    F.col(\"name\"), \n",
    "                    F.lit(\"_augmented_\"), \n",
    "                    F.col(\"pos\").cast(\"string\")\n",
    "                )\n",
    "            ) \\\n",
    "            .withColumn(\"path\", F.lit(\"N/A\")) \\\n",
    "            .select(\"name\", \"path\", \"label\", \"image\") # Re-order and drop temp columns\n",
    "\n",
    "# 7. Execute the *single* write operation\n",
    "\n",
    "# This one write operation executes the entire plan (filter, explode, transform)\n",
    "# in one optimized Spark job.\n",
    "augmented_df.write.mode('overwrite').format('parquet').save(f'{mount_path}/images_anormaly_final')\n",
    "\n",
    "print(f\"Successfully generated and saved {n_augmentations_per_image * num_anormaly} augmented images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7898bb1d-9f0c-4c33-81a6-7a2d94695ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(augmented_df, limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3fbd58-4696-4a53-a7b1-2df5b4541830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_bytes_image_in_df(transposed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e987207-0580-45fb-a14f-3ecf2b959aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "noise_df_final = salt_pepper_df.union(transposed_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_augmentation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "385ed15e-4f99-4b57-8b52-fbeab70401b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 05: Manual Artifact Logging (Plan B)\n",
    "\n",
    "**Purpose:** This notebook is a recovery script. Use it **only if** the main `04_model` training notebook successfully trained a model but **failed to log the model artifact** to MLflow (e.g., the \"Artifacts\" section in the MLflow run is empty).\n",
    "\n",
    "This script will:\n",
    "1.  Re-establish the Spark session's authentication to Azure storage.\n",
    "2.  Re-create the necessary objects (`processor`, `test_set`) that were lost when the cluster terminated.\n",
    "3.  Load the best model from the saved **checkpoint files** on disk.\n",
    "4.  Resume the *existing* MLflow run (using its Run ID).\n",
    "5.  Manually log the model artifact, processor, and signature to that existing run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ad8c56-4a17-4725-8861-90fc78ee61e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Essential Libraries\n",
    "\n",
    "Import only the libraries needed to load data, re-create objects, and log the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9d9ede-3644-4ee9-9629-ebaf0d21641b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Update package lists and install the required AIO library\n",
    "sudo apt-get update && sudo apt-get install -y libaio-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e9612d-d246-4192-9f3d-5a5029a3dd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-identity azure-keyvault-secrets\n",
    "\n",
    "# Pin transformers to a version compatible with our MLflow version (2.21.3)\n",
    "%pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f66d38-3489-4ff7-bda8-f48432a3336a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After installing new libraries, we must restart the Python kernel \n",
    "# for the changes to take effect in the notebook's environment.\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bc359f-a6b7-423e-8df2-c267af6c1f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow, transformers\n",
    "# Verify the MLflow and transformers version.\n",
    "print(mlflow.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c62392dd-8137-4121-ab4c-89a44cc0e3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from PIL import Image\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828cfcbb-ce86-41ef-8101-73f63066cb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Configure Spark Authentication\n",
    "\n",
    "**Reason:** We must re-configure the Spark session's access to Azure storage. This is required because `Dataset.from_spark(train_df)` (in the next step) will launch a new Spark job to read the Delta table, and this job needs permission to access the underlying storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13af7949-c0ec-42bc-9658-227ffc446a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Fetch secrets from Azure Key Vault ---\n",
    "key_vault_url = \"https://anomalykeyvault10222025.vault.azure.net/\"\n",
    "key_name_tenant_id = \"tenant-id\"\n",
    "key_name_client_id = \"client-id\"\n",
    "key_name_client_secret = \"client-secret\"\n",
    "storge_account_name = \"stlabelingdevwestus001\" \n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=key_vault_url, credential=credential)\n",
    "\n",
    "tenant_id = client.get_secret(key_name_tenant_id).value\n",
    "client_id = client.get_secret(key_name_client_id).value\n",
    "client_secret = client.get_secret(key_name_client_secret).value\n",
    "\n",
    "# --- 2. Set Service Principal credentials on the global Spark session ---\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storge_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storge_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storge_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storge_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storge_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "print(f\"Spark session configured for Service Principal access to: {storge_account_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51b0a92-bf18-4fa2-9bc2-93b204658d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Re-create `processor` and `test_set`\n",
    "\n",
    "**Reason:** The original `processor` and `test_set` objects existed only in the memory of the terminated cluster. We must re-create them.\n",
    "* `processor` is required by `mlflow.transformers.log_model` to save the complete pipeline.\n",
    "* `test_set` is required to get a sample input for generating the model's signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e81dec-86d8-4d80-9022-7fa620b42922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Define dataset name and load processor ---\n",
    "dataset_name = \"train_dataset\"\n",
    "processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224', use_fast = True, do_resize = True, size = 224)\n",
    "\n",
    "# --- 2. Define the preprocessing function (must be identical to training) ---\n",
    "def preprocess(example):\n",
    "    image = Image.open(io.BytesIO(example['image'])).convert('RGB')\n",
    "    processed_img = processor(images = image, return_tensors = 'pt')\n",
    "    example['pixel_values'] = processed_img['pixel_values'].squeeze()\n",
    "    return example\n",
    "\n",
    "# --- 3. Re-load and re-process the data ---\n",
    "print(\"Loading Spark DataFrame...\")\n",
    "train_df = spark.read.table(dataset_name)\n",
    "\n",
    "print(\"Converting to Hugging Face Dataset...\")\n",
    "train_dataset = Dataset.from_spark(train_df)\n",
    "\n",
    "print(\"Casting label column...\")\n",
    "class_label_feature = ClassLabel(names=['surfing', 'noise'])\n",
    "train_dataset = train_dataset.cast_column('label', class_label_feature)\n",
    "\n",
    "print(\"Mapping preprocessing function...\")\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "\n",
    "print(\"Setting torch format...\")\n",
    "train_dataset.set_format(type='torch', columns=['pixel_values', 'label'])\n",
    "\n",
    "print(\"Splitting dataset to get 'test_set'...\")\n",
    "train_dataset = train_dataset.train_test_split(test_size = 0.2, stratify_by_column='label')\n",
    "test_set = train_dataset['test']\n",
    "\n",
    "print(\"Processor and test_set are re-created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135c4274-02e9-4f48-a2e6-0691ef1603c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Load from Checkpoint and Log to Existing MLflow Run\n",
    "\n",
    "This is the main recovery step. We will:\n",
    "1.  Define the `run_id` of the failed run (copy this from the MLflow UI).\n",
    "2.  Define the path to the `best_checkpoint_name` (e.g., `checkpoint-26`) saved by the `Trainer`.\n",
    "3.  Load this checkpoint from disk into a model object.\n",
    "4.  Use `mlflow.start_run(run_id=...)` to \"re-open\" the existing MLflow run.\n",
    "5.  Manually generate the model signature.\n",
    "6.  Call `mlflow.transformers.log_model` to finally save the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2658b038-6f0c-4eef-9f16-14f63f1c31fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. User Configuration ---\n",
    "\n",
    "# PASTE THE RUN ID from the MLflow UI\n",
    "run_id_to_resume = \"5b44ff1a1abb4885a01dacec8f233d0e\" \n",
    "\n",
    "# PASTE the name of the best checkpoint folder saved during training\n",
    "best_checkpoint_name = \"checkpoint-26\" \n",
    "\n",
    "# Define paths (ensure these match your training script)\n",
    "model_name = 'vit-base-patch16-224-anomaly'\n",
    "mount_path = \"/mnt/datamount\"\n",
    "output_path = f\"/dbfs{mount_path}/{model_name}\"\n",
    "best_model_path = f\"{output_path}/{best_checkpoint_name}\"\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device set to use {device}\")\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Loading best model from checkpoint: {best_model_path}\")\n",
    "try:\n",
    "    # 2. Load the trained model from the checkpoint file\n",
    "    model_to_log = AutoModelForImageClassification.from_pretrained(best_model_path)\n",
    "    model_to_log.to(device) # Ensure model is on the correct device\n",
    "    model_to_log.eval()     # Set model to evaluation mode\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # 3. Resume the previously failed MLflow Run\n",
    "    with mlflow.start_run(run_id=run_id_to_resume):\n",
    "        \n",
    "        print(f\"Resumed MLflow run: {run_id_to_resume}\")\n",
    "        print(\"Logging model artifacts manually...\")\n",
    "\n",
    "        # 4. Manually create the signature\n",
    "        # Get a sample input tensor from the test_set\n",
    "        sample_input = next(iter(test_set))\n",
    "        input_tensor = sample_input['pixel_values'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Run the model to get a sample output\n",
    "        with torch.no_grad():\n",
    "            output_tensor = model_to_log(input_tensor)\n",
    "\n",
    "        # Convert input and output to numpy for the signature\n",
    "        input_array = input_tensor.cpu().numpy()\n",
    "        output_array = output_tensor.logits.cpu().numpy()\n",
    "\n",
    "        # Create the signature object\n",
    "        signature = infer_signature(input_array, output_array)\n",
    "        \n",
    "        # 5. Log the model artifacts\n",
    "        mlflow.transformers.log_model(\n",
    "            transformers_model={\n",
    "                \"model\": model_to_log,\n",
    "                \"image_processor\": processor # Use the 'image_processor' key\n",
    "            }, \n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,          # Use the manual signature\n",
    "            task=\"image-classification\"   # Explicitly state the task\n",
    "        )\n",
    "        print(\"Model artifacts logged successfully to existing run.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to load model or log artifacts: {e}\")\n",
    "\n",
    "print(\"Manual logging process finished.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6877237428234726,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_manual_artifact_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
